{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script (1) compiles published literature and (2) generates areas of interest for 3DEP downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "plt.rcParams.update({\"pdf.fonttype\":42})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather all geologic maps for Ordovician and Silurian rocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_s_polys = gpd.GeoDataFrame()\n",
    "\n",
    "for state in glob.glob(\"./geology/*_geol_poly.shp\"):\n",
    "    state_geol = gpd.read_file(state)\n",
    "    state_o_s = state_geol[(state_geol['ORIG_LABEL'].str.contains('S')) | (state_geol['ORIG_LABEL'].str.contains('O'))]\n",
    "    o_s_polys = gpd.GeoDataFrame(pd.concat([o_s_polys, state_o_s], ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liths = pd.DataFrame()\n",
    "\n",
    "for state in glob.glob(\"./geology/*_lith.csv\"):\n",
    "    state_liths = pd.read_csv(state)\n",
    "    liths = pd.concat([liths, state_liths], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = pd.DataFrame()\n",
    "\n",
    "for state in glob.glob(\"./geology/*_units.csv\"):\n",
    "    state_units = pd.read_csv(state)\n",
    "    units = pd.concat([units, state_units], ignore_index=True)\n",
    "\n",
    "descripts = pd.merge(units, liths, on=\"unit_link\")\n",
    "\n",
    "all_data = pd.merge(o_s_polys, descripts, left_on=\"UNIT_LINK\", right_on=\"unit_link\")\n",
    "all_data = all_data.dissolve(by='ORIG_LABEL').reset_index()\n",
    "# all_data.to_file(\"all_data.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find large-scale trends in relief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make latitude bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import numpy as np\n",
    "\n",
    "ymin=31\n",
    "ymax=45\n",
    "xmin=-88\n",
    "xmax=-73\n",
    "width = abs(xmin)-abs(xmax)\n",
    "height = 0.125\n",
    "rows = int(np.ceil((ymax-ymin) /  height))\n",
    "cols = int(np.ceil((xmax-xmin) / width))\n",
    "XleftOrigin = xmin\n",
    "XrightOrigin = xmin + width\n",
    "YtopOrigin = ymax\n",
    "YbottomOrigin = ymax- height\n",
    "polygons = []\n",
    "for i in range(cols):\n",
    "    Ytop = YtopOrigin\n",
    "    Ybottom =YbottomOrigin\n",
    "for j in range(rows):\n",
    "    polygons.append(Polygon([(XleftOrigin, Ytop), (XrightOrigin, Ytop), (XrightOrigin, Ybottom), (XleftOrigin, Ybottom)])) \n",
    "    Ytop = Ytop - height\n",
    "    Ybottom = Ybottom - height\n",
    "XleftOrigin = XleftOrigin + width\n",
    "XrightOrigin = XrightOrigin + width\n",
    "\n",
    "grid = gpd.GeoDataFrame({'geometry':polygons}).set_crs(\"EPSG:4326\")\n",
    "\n",
    "# https://gis.stackexchange.com/questions/269243/creating-polygon-grid-using-geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid['lat_max'] = [(x.exterior.coords)[2][1] for x in polygons]\n",
    "grid['lat_min'] = [(x.exterior.coords)[0][1] for x in polygons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_by_lat_bin = gpd.overlay(all_data, grid, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuscarora = all_data_by_lat_bin[all_data_by_lat_bin['unit_name'].str.contains(\"Tuscarora\")].to_crs('EPSG:26917')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get relief raster and do some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio import features\n",
    "from shapely.geometry import box, shape\n",
    "\n",
    "with rasterio.open(\"./topo/Central_App_relief_2500m_resample1.tif\", masked=True) as relief:\n",
    "    print(relief.crs)\n",
    "    relief_meta = relief.profile\n",
    "    relief_arr = relief.read(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Tuscarora relief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_dict = {}\n",
    "raw_values_dict = {}\n",
    "\n",
    "for geom, idx in zip(tuscarora['geometry'], tuscarora.index):\n",
    "    # I spent WAY too much time messing around with this part, you apparently can't just\n",
    "    # give rasterize a polygon, it has to either be a MutliPolygon or a list of geometries\n",
    "    # This code does the latter, and the Arctic Data Center tutorial's only works because their\n",
    "    # example vector data is accidentally multipolygons, but our data has a polygon\n",
    "    geom = shape(geom)\n",
    "    geoms = [(geom, 1)]\n",
    "    # Now this looks like the ADC example\n",
    "    rasterized = features.rasterize(\n",
    "                                    geoms,\n",
    "                                    out_shape=relief_arr.shape, # Look like the source raster\n",
    "                                    transform=relief_meta['transform'], # Have the geometry of the source raster\n",
    "                                    all_touched=True # all pixels touched by geometries (as opposed to pixel centers)\n",
    "                                    )\n",
    "    # Theoretically instead of individually rasterizing each polygon type\n",
    "    # You could rasterize the whole thing and instead of 0s and 1s you can\n",
    "    # make the raster value the index and then do basic array-style analyses\n",
    "    # Maybe I can offer treats to someone who writes that for me...\n",
    "    r_index = np.where(rasterized == 1) # Make the mask\n",
    "    raw_values_dict[idx] = relief_arr[r_index] # store all non-masked data (the compressed thing)\n",
    "    means_dict[idx] = np.nanmean(relief_arr[r_index]) # calculate the mean of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_df = pd.DataFrame.from_dict(means_dict,\n",
    "                                     orient='index',\n",
    "                                     columns=['mean_relief'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuscarora_relief = tuscarora.merge(means_df,\n",
    "                                    left_index=True,\n",
    "                                    right_index=True,\n",
    "                                    how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get relief by province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "province_relief = {}\n",
    "provinces = gpd.read_file(\"./cartography/physio.shp\").to_crs('EPSG:4326').dissolve(by='PROVINCE').reset_index()\n",
    "# province_list = list(provinces['PROVINCE'].unique())\n",
    "\n",
    "province_list = [\n",
    " 'APPALACHIAN PLATEAUS',\n",
    " 'BLUE RIDGE',\n",
    "#  'COASTAL PLAIN',\n",
    "#  'PIEDMONT',\n",
    " 'VALLEY AND RIDGE',\n",
    "]\n",
    "\n",
    "for province in province_list:\n",
    "    subdict = {}\n",
    "    prov_by_lat_bin = gpd.overlay(provinces[provinces['PROVINCE']==province], grid, how='intersection').to_crs('EPSG:26917')\n",
    "    subdict['overlay'] = prov_by_lat_bin\n",
    "\n",
    "    means_dict = {}\n",
    "    # raw_values_dict = {}\n",
    "\n",
    "    for geom, idx in zip(prov_by_lat_bin['geometry'], prov_by_lat_bin.index):\n",
    "        # I spent WAY too much time messing around with this part, you apparently can't just\n",
    "        # give rasterize a polygon, it has to either be a MutliPolygon or a list of geometries\n",
    "        # This code does the latter, and the Arctic Data Center tutorial's only works because their\n",
    "        # example vector data is accidentally multipolygons, but our data has a polygon\n",
    "        geom = shape(geom)\n",
    "        geoms = [(geom, 1)]\n",
    "        # Now this looks like the ADC example\n",
    "        rasterized = features.rasterize(\n",
    "                                        geoms,\n",
    "                                        out_shape=relief_arr.shape, # Look like the source raster\n",
    "                                        transform=relief_meta['transform'], # Have the geometry of the source raster\n",
    "                                        all_touched=True # all pixels touched by geometries (as opposed to pixel centers)\n",
    "                                        )\n",
    "        # Theoretically instead of individually rasterizing each polygon type\n",
    "        # You could rasterize the whole thing and instead of 0s and 1s you can\n",
    "        # make the raster value the index and then do basic array-style analyses\n",
    "        # Maybe I can offer treats to someone who writes that for me...\n",
    "        r_index = np.where(rasterized == 1) # Make the mask\n",
    "        # r_index.filled(np.nan)\n",
    "        # raw_values_dict[idx] = relief_arr[r_index] # store all non-masked data (the compressed thing)\n",
    "        means_dict[idx] = np.nanmean(relief_arr[r_index]) # calculate the mean of the data\n",
    "\n",
    "    means_df = pd.DataFrame.from_dict(means_dict,\n",
    "                                        orient='index',\n",
    "                                        columns=['mean_relief'])\n",
    "\n",
    "    prov_relief = prov_by_lat_bin.merge(means_df,\n",
    "                                        left_index=True,\n",
    "                                        right_index=True,\n",
    "                                        how='inner')\n",
    "    subdict['relief'] = prov_relief.dropna()\n",
    "    subdict['relief'] = prov_relief.loc[prov_relief['mean_relief'] < 1001, :]\n",
    "\n",
    "    province_relief[province] = subdict\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "for province in province_list:\n",
    "    subdict = {}\n",
    "    prov_by_lat_bin = gpd.overlay(provinces[provinces['PROVINCE']==province], grid, how='intersection').to_crs('EPSG:26917')\n",
    "    subdict['overlay'] = prov_by_lat_bin\n",
    "\n",
    "    means_dict = {}\n",
    "    # raw_values_dict = {}\n",
    "\n",
    "    for geom, idx in zip(prov_by_lat_bin['geometry'], prov_by_lat_bin.index):\n",
    "        # I spent WAY too much time messing around with this part, you apparently can't just\n",
    "        # give rasterize a polygon, it has to either be a MutliPolygon or a list of geometries\n",
    "        # This code does the latter, and the Arctic Data Center tutorial's only works because their\n",
    "        # example vector data is accidentally multipolygons, but our data has a polygon\n",
    "        geom = shape(geom)\n",
    "        geoms = [(geom, 1)]\n",
    "        # Now this looks like the ADC example\n",
    "        rasterized = features.rasterize(\n",
    "                                        geoms,\n",
    "                                        out_shape=relief_arr.shape, # Look like the source raster\n",
    "                                        transform=relief_meta['transform'], # Have the geometry of the source raster\n",
    "                                        all_touched=True # all pixels touched by geometries (as opposed to pixel centers)\n",
    "                                        )\n",
    "        # Theoretically instead of individually rasterizing each polygon type\n",
    "        # You could rasterize the whole thing and instead of 0s and 1s you can\n",
    "        # make the raster value the index and then do basic array-style analyses\n",
    "        # Maybe I can offer treats to someone who writes that for me...\n",
    "        r_index = np.where(rasterized == 1) # Make the mask\n",
    "        # r_index.filled(np.nan)\n",
    "        # raw_values_dict[idx] = relief_arr[r_index] # store all non-masked data (the compressed thing)\n",
    "        means_dict[idx] = np.nanmean(relief_arr[r_index]) # calculate the mean of the data\n",
    "\n",
    "    means_df = pd.DataFrame.from_dict(means_dict,\n",
    "                                        orient='index',\n",
    "                                        columns=['mean_relief'])\n",
    "\n",
    "    prov_relief = prov_by_lat_bin.merge(means_df,\n",
    "                                        left_index=True,\n",
    "                                        right_index=True,\n",
    "                                        how='inner')\n",
    "    subdict['relief'] = prov_relief.dropna()\n",
    "    subdict['relief'] = prov_relief.loc[prov_relief['mean_relief'] < 1001, :]\n",
    "\n",
    "    province_relief[province] = subdict\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "for province in province_list:\n",
    "    subdict = {}\n",
    "    prov_by_lat_bin = gpd.overlay(provinces[provinces['PROVINCE']==province], grid, how='intersection').to_crs('EPSG:26917')\n",
    "    subdict['overlay'] = prov_by_lat_bin\n",
    "\n",
    "    means_dict = {}\n",
    "    # raw_values_dict = {}\n",
    "\n",
    "    for geom, idx in zip(prov_by_lat_bin['geometry'], prov_by_lat_bin.index):\n",
    "        # I spent WAY too much time messing around with this part, you apparently can't just\n",
    "        # give rasterize a polygon, it has to either be a MutliPolygon or a list of geometries\n",
    "        # This code does the latter, and the Arctic Data Center tutorial's only works because their\n",
    "        # example vector data is accidentally multipolygons, but our data has a polygon\n",
    "        geom = shape(geom)\n",
    "        geoms = [(geom, 1)]\n",
    "        # Now this looks like the ADC example\n",
    "        rasterized = features.rasterize(\n",
    "                                        geoms,\n",
    "                                        out_shape=relief_arr.shape, # Look like the source raster\n",
    "                                        transform=relief_meta['transform'], # Have the geometry of the source raster\n",
    "                                        all_touched=True # all pixels touched by geometries (as opposed to pixel centers)\n",
    "                                        )\n",
    "        # Theoretically instead of individually rasterizing each polygon type\n",
    "        # You could rasterize the whole thing and instead of 0s and 1s you can\n",
    "        # make the raster value the index and then do basic array-style analyses\n",
    "        # Maybe I can offer treats to someone who writes that for me...\n",
    "        r_index = np.where(rasterized == 1) # Make the mask\n",
    "        # r_index.filled(np.nan)\n",
    "        # raw_values_dict[idx] = relief_arr[r_index] # store all non-masked data (the compressed thing)\n",
    "        # means_dict[idx] = np.nanmean(relief_arr[r_index]) # calculate the mean of the data\n",
    "        if relief_arr[r_index].size:\n",
    "            means_dict[idx] = np.nanmean(relief_arr[r_index]) # calculate the mean of the data\n",
    "\n",
    "    means_df = pd.DataFrame.from_dict(means_dict,\n",
    "                                        orient='index',\n",
    "                                        columns=['mean_relief'])\n",
    "\n",
    "    prov_relief = prov_by_lat_bin.merge(means_df,\n",
    "                                        left_index=True,\n",
    "                                        right_index=True,\n",
    "                                        how='inner')\n",
    "    subdict['relief'] = prov_relief.dropna()\n",
    "    subdict['relief'] = prov_relief.loc[prov_relief['mean_relief'] < 1001, :]\n",
    "\n",
    "    province_relief[province] = subdict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(12,3), dpi=200)\n",
    "# sns.scatterplot(x='lat_min', y='mean_relief', data=tuscarora_relief, hue='ORIG_LABEL')\n",
    "for province in list(province_relief.keys()):\n",
    "    sns.lineplot(x='lat_min', y='mean_relief', data=province_relief[province]['relief'], label=province)\n",
    "sns.scatterplot(x='lat_min', y='mean_relief', data=tuscarora_relief, hue='ORIG_LABEL')\n",
    "ax.set_ylabel('Mean relief (m)')\n",
    "ax.set_xlim(36.5, 41.5)\n",
    "plt.savefig(\"mean_relief_lat_bin_tuscarora.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,3), dpi=200)\n",
    "# sns.scatterplot(x='lat_min', y='mean_relief', data=tuscarora_relief, hue='ORIG_LABEL')\n",
    "for province in list(province_relief.keys()):\n",
    "    sns.lineplot(x='lat_min', y='mean_relief', data=province_relief[province]['relief'], label=province)\n",
    "sns.scatterplot(x='lat_min', y='mean_relief', data=tuscarora_relief, hue='ORIG_LABEL')\n",
    "ax.set_ylabel('Mean relief (m)')\n",
    "ax.set_xlim(36.5, 41.5)\n",
    "ax.set_ylim(600,50)\n",
    "plt.savefig(\"mean_relief_lat_bin_tuscarora_r.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do the (Paul) numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a marketplace joke"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019 potomac compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdr1 = pd.read_csv(\"./be10/portenga_dr1.csv\")\n",
    "pdr3 = pd.read_csv(\"./be10/portenga_dr3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portenga_2019 = pd.merge(pdr1, pdr3, left_on=\"Sample Ida\", right_on=\"Sample IDa\", how=\"outer\")\n",
    "portenga_2019 = portenga_2019.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n",
    "portenga_2019['Long (dd)'] = portenga_2019['Longitude (째W)'].values*-1\n",
    "portenga_2019.loc[:,'published_in'] = 'Portegna et al 2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portenga_2019.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gsa today 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p11_basin = pd.read_csv(\"./be10/portenga_comp_basins.csv\")\n",
    "p11_outcrop = pd.read_csv(\"./be10/portenga_comp_outcrops.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portenga_2011 = pd.concat([p11_basin, p11_outcrop]).dropna(how=\"all\")\n",
    "portenga_2011.loc[:,'published_in'] = 'Portegna et al 2011'\n",
    "portenga_2011['Sample type']=portenga_2011['Sample type'].fillna(\"Basin\")\n",
    "pd.to_numeric(portenga_2011['Lat (dd)'], errors=\"coerce\")\n",
    "portenga_2011['lat_bins'] = pd.cut(portenga_2011['Lat (dd)'], np.arange(30.5,42.0,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portenga_2011.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 2019 data (first) to the 2011 compilation (second)\n",
    "column_dict = {'Sample Ida' : 'Sample ID',\n",
    "'Latitude (째N)' : 'Lat (dd)',\n",
    "# 'Longitude (째W)' : 'Long (dd)',\n",
    "'Unnested 10Bei erosion rate  (m Myr-1)b':'Published erosion rate',\n",
    "'Avg. basin slope (째)c':'Basin slope',\n",
    "'Basin area (km2)':'Basin Area',\n",
    "'Relief ' : 'Basin relief (m)',\n",
    "'published_in':'published_in'\n",
    "}\n",
    "\n",
    "portenga_all = pd.concat([portenga_2011, portenga_2019.rename(columns=column_dict)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portenga_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portenga_all=portenga_all.apply(pd.to_numeric,\n",
    "# #  errors=\"ignore\"\n",
    "#  )\n",
    "portenga_all['Published erosion rate'] = pd.to_numeric(portenga_all['Published erosion rate'], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn the sites into a geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portenga_gdf = gpd.GeoDataFrame(\n",
    "    portenga_all, geometry=gpd.points_from_xy(portenga_all['Long (dd)'], portenga_all['Lat (dd)']), crs='epsg:4326')\n",
    "\n",
    "portenga_gdf = portenga_gdf.dropna(subset='Citation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Portenga data to geospatial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join to provinces map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portenga_gdf = gpd.sjoin(portenga_gdf, provinces).drop(columns=['index_right'])\n",
    "portenga_gdf['MAP'] = portenga_gdf['MAP'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make squares for 3DEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is so that I can get tiles for 3DEP lidar data for LSDTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = portenga_gdf[portenga_gdf['PROVINCE']=='VALLEY AND RIDGE'].filter(['Sample ID', 'geometry'])\n",
    "\n",
    "def points_to_squares(gdf):\n",
    "    # Takes lat long points\n",
    "    # makes utm boxes\n",
    "    # sends them back to lat long but boxes\n",
    "    gdf['EPSG']= (\n",
    "        32700-(np.round((45+gdf.geometry.y)/90,0)*100).astype(int)\n",
    "        +np.round((183+gdf.geometry.x)/6,0).astype(int)\n",
    "        ).astype(str)\n",
    "\n",
    " \n",
    "    gdf['square_geom'] = np.nan\n",
    "    for code in gdf['EPSG'].unique():\n",
    "        code_subset = gdf.loc[gdf['EPSG'] == code]\n",
    "        square = code_subset.to_crs(f'EPSG:{code}').buffer(1000, cap_style=3).to_crs(\"EPSG:4326\")\n",
    "        gdf.loc[gdf['EPSG'] == code, 'square_geom'] = square.geometry\n",
    "        # list_4326.append([square.geometry])\n",
    "\n",
    "    gdf_squares = gpd.GeoDataFrame(\n",
    "        gdf.drop('geometry', axis=1).copy(), geometry=gdf.square_geom, crs=\"EPSG:4326\"\n",
    "        ).drop('square_geom', axis=1)\n",
    "    \n",
    "    return gdf_squares\n",
    "\n",
    "portenga_squares = points_to_squares(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join 10Be data to geology map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = gpd.sjoin(\n",
    "    portenga_gdf, all_data,\n",
    " how=\"left\"\n",
    "\n",
    " ).drop('index_right',\n",
    "#  'index_left'],\n",
    " axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize 10Be data with geology and morphometry attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined['lat_stat'] = pd.cut(joined.geometry.y, [0,39,49])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I report these numbers in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined[joined['PROVINCE']=='VALLEY AND RIDGE']['Citation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get morph attributes from publishd and from LSDTT output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_basins = joined.loc[joined['PROVINCE']=='VALLEY AND RIDGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_ridgelines = pd.read_csv('output_data/portenga_ridgelines_Cht.csv')\n",
    "# vr_ridgelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sites that are super close together and make a site average\n",
    "\n",
    "numerical_cols = vr_ridgelines.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "grouped_numerical = vr_ridgelines.groupby('Location')[numerical_cols].mean().reset_index()\n",
    "grouped_numerical\n",
    "\n",
    "vr_ridgelines_0 = pd.merge(grouped_numerical, vr_ridgelines.select_dtypes(exclude=['number']).groupby('Location').first().reset_index(),how='left')\n",
    "vr_ridgelines= vr_ridgelines_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_ridgelines = vr_ridgelines.loc[vr_ridgelines['PROVINCE']=='VALLEY AND RIDGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_basins.loc[:,'sample_cat'] = 'basin'\n",
    "vr_ridgelines.loc[:,'sample_cat'] = 'ridgelines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_for_boxplots = pd.concat([vr_basins, vr_ridgelines])\n",
    "merged_for_boxplots['lat_bins'] = pd.cut(merged_for_boxplots['Lat (dd)'], np.arange(37.5,42.0,0.5))\n",
    "\n",
    "fig, ax=plt.subplots(figsize=(1,4),dpi=300)\n",
    "sns.boxplot(data=merged_for_boxplots, y='lat_bins', x='CRONUS', hue='sample_cat', ax=ax, legend=False)\n",
    "plt.yticks(rotation=45)\n",
    "ax.invert_yaxis()\n",
    "plt.savefig(\"portenga_vs_lat_all.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "limit basins to 5 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_basins = vr_basins[vr_basins['Basin Area']<5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_lat = 39\n",
    "\n",
    "vr_basins['lat_stat'] = pd.cut(vr_basins.geometry.y, [0,switch_lat,49])\n",
    "\n",
    "vr_ridgelines['lat_stat'] = pd.cut(vr_ridgelines['Lat (dd)'], [0,switch_lat,49])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make clean versions for DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_basins.to_csv('delvecchio_DR_basins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(vr_ridgelines, portenga_all, how='left', on='Sample ID').dropna(axis=1, how='all').to_csv('delvecchio_DR_ridgelines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,1, \n",
    "sharey=True,\n",
    "figsize = (\n",
    "  # 4.83,\n",
    "  2.33,\n",
    "  4), dpi=200)\n",
    "sns.scatterplot(data=vr_ridgelines, \n",
    "                x=vr_ridgelines['Cht']*-1,\n",
    "                 y='CRONUS', \n",
    "                #  y=pd.cut(vr_ridgelines['CRONUS'], e_bins),\n",
    "                 hue='lat_stat', ax=ax[0], legend=False)\n",
    "sns.scatterplot(data=vr_basins, x='Basin slope', \n",
    "                y='CRONUS',\n",
    "                # y=pd.cut(vr_ridgelines['CRONUS'], e_bins),\n",
    "                  hue='lat_stat', ax=ax[1], legend=False)\n",
    "# ax[0].set_xlim(0.0025,0.03)\n",
    "# ax[0].set_xscale('log')\n",
    "ax[0].set_xticks([0.001, 0.01,0.02])\n",
    "\n",
    "ax[0].set_xscale('log')\n",
    "\n",
    "\n",
    "\n",
    "# ax[0].invert_xaxis()\n",
    "ax[0].set_xlabel('-Cht')\n",
    "\n",
    "\n",
    "# labels = [item.get_text() for item in ax[0].get_xticklabels()]\n",
    "# labels = [f'{float(label):.2f}' for label in labels] # Format to 2 decimal places\n",
    "# ax[0].set_xticklabels(labels)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[0].xaxis.set_major_formatter(ScalarFormatter())\n",
    "ax[0].xaxis.get_major_formatter().set_scientific(False)\n",
    "ax[0].xaxis.get_major_formatter().set_useOffset(False)\n",
    "\n",
    "plt.savefig('figure_outputs/morph_erates.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geobasic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
